{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.optimize\n",
    "import os\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6: Multiple View Geometry\n",
    "\n",
    "In the lst couple of weeks we've talked about multiple-view geometry. The main pursuits in this domain are:\n",
    "* Reconstructing the 3D geometry of the objects in the visible scene\n",
    "  * Dense reconstruction with stereo\n",
    "  * Sparse reconstruction with feature key-points (e.g corners)\n",
    "* Estimating the pose (location, orientation) of the cameras looking at the scene\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with images from a well known Structure-from-Motion dataset: https://cvlab.epfl.ch/data/data-strechamvs/\n",
    "\n",
    "Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_IDS = [\n",
    "    '1G9NS27L2YEgMUzU34iquIm-A5MBlYiwr',\n",
    "    '1gR65HLkcAcksolu3cU46MxPY4cn_V8Mw',\n",
    "    '1CLgiVSltl69dYmZOQV9Zt7wfWFTAochn',\n",
    "    '1WiuVyg4btUJQU_1XOhAOwpYWjlATuyaw',\n",
    "    '1XYWtRQJ6REJEpEgza0tEoIXNNLTB2k88',\n",
    "    '1661OnKt8Ns5KvWiy6FSI95G7NXVnKOCh',\n",
    "    '177YlElwbg8vlPiLzk5P-JKD4Ce3a7vs7',\n",
    "    '1Ucr5vDIzUwdaxde5f_gdk2FYH3jl8suV',\n",
    "    '1M6j1fdKcgULUFqhy6kCVwOQUOFtzref7',\n",
    "    '1CQpPpefDgxv5DDGYprmvlNK_cq-hNqDi',\n",
    "    '1Bx3AJo9q_ttZ-qv3ZZT1B_9lJeO5kguz',\n",
    "    ]\n",
    "images = []\n",
    "for dl_id in DOWNLOAD_IDS:\n",
    "    filename = f\"image{dl_id}.jpg\"\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(requests.get(\"https://drive.google.com/uc?id=%s\"%(dl_id)).content)\n",
    "    images.append(cv2.resize(cv2.imread(filename, cv2.IMREAD_COLOR)[...,::-1], (0,0), fx=0.25, fy=0.25))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Graph\n",
    "To get started with an MVG pipeline we need to extract features and descriptors from images.\n",
    "\n",
    "Below is a class that I wrote to help with some of this work so you can focus on the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MatchMaker class is a helper class to store the keypoints matches and match graph\n",
    "# and provide some helper functions to get robust, aligned matches between two images.\n",
    "# I'm providing this class to you to speed up our process, but you should read through\n",
    "# it to understand how it works roughly.\n",
    "class MatchMaker:\n",
    "    def __init__(self) -> None:\n",
    "        self.detector = cv2.SIFT_create()\n",
    "        self.matcher = cv2.FlannBasedMatcher(\n",
    "            dict(algorithm=1, trees=5), dict(checks=50))\n",
    "        self.images = []\n",
    "        self.kpts = []\n",
    "        self.descs = []\n",
    "        self.matches = {}\n",
    "        self.kpts_match_graph = None\n",
    "        self.point3d_camera_visibility = None\n",
    "        self.map_3d = None\n",
    "        self.poses = None\n",
    "        pass\n",
    "\n",
    "    def getMatchesFilterFundamental(self, left_image_index, right_image_index):\n",
    "        matches_raw = list(self.matcher.knnMatch(\n",
    "            self.descs[left_image_index], self.descs[right_image_index], 2))\n",
    "        matches_ = []\n",
    "        for (m, n) in matches_raw:\n",
    "            if m.distance < 0.7*n.distance:\n",
    "                matches_.append(m)\n",
    "\n",
    "        # filter by finding the fundamental matrix with RANSAC\n",
    "        mptsif, mptsjf = zip(*[(self.kpts[left_image_index][m.queryIdx].pt,\n",
    "                                self.kpts[right_image_index][m.trainIdx].pt)\n",
    "                               for m in matches_])\n",
    "        mptsif, mptsjf = np.array(mptsif), np.array(mptsjf)\n",
    "        _, mask = cv2.findFundamentalMat(mptsif, mptsjf, cv2.FM_RANSAC)\n",
    "        matches_ = [matches_[i] for i in range(len(matches_)) if mask[i]]\n",
    "\n",
    "        return np.array(matches_)\n",
    "\n",
    "    def addImagesAndExtractKeypoints(self, images):\n",
    "        self.images = images\n",
    "        self.kpts, self.descs = zip(\n",
    "            *[self.detector.detectAndCompute(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), None) for img in images])\n",
    "        # store the 3D points visibility graph in a 2D array\n",
    "        self.point3d_camera_visibility = -np.ones((len(self.images), 500), np.int32)\n",
    "        # camera poses\n",
    "        self.poses = np.zeros((len(self.images), 3, 4))\n",
    "\n",
    "    def buildMatchGraph(self):\n",
    "        # Match every image with every other image (without repetition).\n",
    "        # For each pair of images, get the matches and fundamental matrix using the helper function.\n",
    "        # store in a dictionary with key (i,j) and value (mptsif,mptsjf,inliers,f_ij)\n",
    "        self.matches = {}\n",
    "        # store the match graph in a 2D array\n",
    "        self.kpts_match_graph = -np.ones((len(self.images), len(self.images), np.max(\n",
    "            [len(kptsi) for kptsi in self.kpts])), dtype=np.int32)\n",
    "        \n",
    "        for i, j in product(range(len(images)), repeat=2):\n",
    "            if i < j:\n",
    "                self.matches[(i, j)] = self.getMatchesFilterFundamental(i, j)\n",
    "\n",
    "                # update the match graph kpts_match_graph\n",
    "                for m in self.matches[(i, j)]:\n",
    "                    self.kpts_match_graph[i, j, m.queryIdx] = m.trainIdx # right view index\n",
    "                    self.kpts_match_graph[j, i, m.trainIdx] = m.queryIdx # left view index\n",
    "\n",
    "    def getMatchGraph(self):\n",
    "        if self.kpts_match_graph is None:\n",
    "            raise Exception(\"The match graph is not computed yet.\")\n",
    "        return self.kpts_match_graph\n",
    "\n",
    "    def aligned2D(self, left_image_index, right_image_index):\n",
    "        ptsl2d, ptsr2d = zip(*[(self.kpts[left_image_index][m.queryIdx].pt,\n",
    "                                self.kpts[right_image_index][m.trainIdx].pt)\n",
    "                               for m in self.matches[(left_image_index, right_image_index)]])\n",
    "        return np.array(ptsl2d), np.array(ptsr2d)\n",
    "    \n",
    "    def aligned2DNotInMap(self, left_image_index, right_image_index):\n",
    "        ptsl2d, ptsr2d, backidxL, backidxR = zip(*[(self.kpts[left_image_index][m.queryIdx].pt,\n",
    "                                                    self.kpts[right_image_index][m.trainIdx].pt,\n",
    "                                                    m.queryIdx, \n",
    "                                                    m.trainIdx)\n",
    "                               for m in self.matches[(left_image_index, right_image_index)]\n",
    "                                 if not m.queryIdx in self.point3d_camera_visibility[left_image_index] and \n",
    "                                    not m.trainIdx in self.point3d_camera_visibility[right_image_index]\n",
    "                               ])\n",
    "        return np.array(ptsl2d), np.array(ptsr2d), np.array(backidxL), np.array(backidxR)\n",
    "    \n",
    "    def alignedIndices(self, left_image_index, right_image_index):\n",
    "        indl2d, indr2d = zip(*[(m.queryIdx, m.trainIdx)\n",
    "                               for m in self.matches[(left_image_index, right_image_index)]])\n",
    "        return np.array(indl2d), np.array(indr2d)\n",
    "\n",
    "    def addNewPoints3D(self, pts3d, li, rj, mask=None):\n",
    "        if self.map_3d is None:\n",
    "            self.map_3d = np.zeros((0, 3))\n",
    "        if mask is None:\n",
    "            mask = np.ones(pts3d.shape[0], dtype=np.bool)\n",
    "\n",
    "        mapIS = self.map_3d.shape[0]\n",
    "        mapIE = mapIS + pts3d[mask].shape[0]\n",
    "        alignedIdxL, alignedIdxR = self.alignedIndices(li, rj)\n",
    "        self.point3d_camera_visibility[li, mapIS:mapIE] = alignedIdxL[mask]\n",
    "        self.point3d_camera_visibility[rj, mapIS:mapIE] = alignedIdxR[mask]\n",
    "        self.map_3d = np.concatenate([self.map_3d, pts3d[mask]])\n",
    "\n",
    "    def R(self, i):\n",
    "        return self.poses[i, :3, :3]\n",
    "    \n",
    "    def t(self, i):\n",
    "        return self.poses[i, :3, 3]\n",
    "    \n",
    "    # get 2D keypoints for a given image (rj, \"right\") which match 3D points in \n",
    "    # the map from another image (li, \"left\")\n",
    "    def alignedKptsTo3DMap(self, li_3d, rj_2d):\n",
    "        # left view indices from 3D points on the current map\n",
    "        li_3d_idx = self.point3d_camera_visibility[li_3d]\n",
    "        # right view indices that align to the left view indices above\n",
    "        indices_rj = self.kpts_match_graph[li_3d, rj_2d, li_3d_idx]\n",
    "        # get the keypoints from the right view\n",
    "        selected_rj_kpts = np.array(self.kpts[rj_2d])[indices_rj[indices_rj > -1]]\n",
    "        # get the keypoints from the left view\n",
    "        selected_li_kpts = np.array(self.kpts[li_3d])[li_3d_idx[li_3d_idx > -1]]\n",
    "        return selected_li_kpts, selected_rj_kpts\n",
    "\n",
    "    # get 2D points for a given image (rj, \"right\") with corresponding 3D \n",
    "    # points in the map from any other image (li, \"left\")\n",
    "    def aligned2D3D(self, rj_2d):\n",
    "        mpts2Drj = []\n",
    "        mpts3D = []\n",
    "        backmapping = []\n",
    "\n",
    "        # for each 3D point in the map\n",
    "        for p3d_id in range(self.map_3d.shape[0]):\n",
    "            # check its visibility in all the left views\n",
    "            for li_3d in range(self.point3d_camera_visibility.shape[0]):\n",
    "                if li_3d != rj_2d: # skip the right view...\n",
    "                    # the index of the 3D point in the left view\n",
    "                    li_3d_kpt_idx = self.point3d_camera_visibility[li_3d, p3d_id]\n",
    "                    if li_3d_kpt_idx < 0: \n",
    "                        continue # this 3D point is not visible in the left view\n",
    "                    rj_2d_kpt_idx = self.kpts_match_graph[li_3d, rj_2d, li_3d_kpt_idx]\n",
    "                    if rj_2d_kpt_idx > -1: # this 3D point is visible in both views\n",
    "                        pt = self.kpts[rj_2d][rj_2d_kpt_idx].pt\n",
    "                        mpts2Drj.append((pt[0], pt[1], 1.0))\n",
    "                        mpts3D.append(self.map_3d[p3d_id])\n",
    "                        backmapping.append((p3d_id, rj_2d_kpt_idx))\n",
    "                        break\n",
    "                        \n",
    "        # return an interleaved array of 2D and 3D points\n",
    "        return np.stack([mpts2Drj, mpts3D], axis=1), backmapping\n",
    "    \n",
    "    def alignedMapTo2DAndVisibility(self):\n",
    "        # return an aligned list of 2D image points and a list of 3D points from the map\n",
    "        mpts2DForViews = np.zeros((self.point3d_camera_visibility.shape[0], self.map_3d.shape[0], 2), dtype=np.float32)\n",
    "        visibility = np.zeros((self.point3d_camera_visibility.shape[0], self.map_3d.shape[0]), dtype=np.uint8)\n",
    "\n",
    "        # for each 3D point in the map\n",
    "        for p3d_id in range(self.map_3d.shape[0]):\n",
    "            # check its visibility in all the left views\n",
    "            for view_i in range(self.point3d_camera_visibility.shape[0]):\n",
    "                # the index of the 3D point in the view\n",
    "                li_3d_kpt_idx = self.point3d_camera_visibility[view_i, p3d_id]\n",
    "                if li_3d_kpt_idx < 0: \n",
    "                    # this 3D point is not visible in the view\n",
    "                    mpts2DForViews[view_i, p3d_id] = [0, 0]\n",
    "                    visibility[view_i, p3d_id] = 0\n",
    "                else:\n",
    "                    # this 3D point is visible in the view\n",
    "                    pt = self.kpts[view_i][li_3d_kpt_idx].pt\n",
    "                    mpts2DForViews[view_i, p3d_id] = [pt[0], pt[1]]\n",
    "                    visibility[view_i, p3d_id] = 1\n",
    "\n",
    "        return self.map_3d, mpts2DForViews, visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the MatchMaker class and extract keypoints from the images\n",
    "mm = MatchMaker()\n",
    "mm.addImagesAndExtractKeypoints(images)\n",
    "mm.buildMatchGraph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the camera intrinsic matrix. This is given to us, and guranteed to be similar and cosistent across images of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K is the camera matrix, given to us by the dataset provider\n",
    "K_original = np.array([[2759.48, 0, 1520.69],[0, 2764.16, 1006.81],[0, 0, 1]])\n",
    "# W and H are the width and height of the images\n",
    "W,H = mm.images[0].shape[1], mm.images[0].shape[0]\n",
    "# K_s is the scaled camera matrix, we use it because we scaled the images to 0.25 their original size\n",
    "K, _ = cv2.getOptimalNewCameraMatrix(K_original, np.zeros(5), (W*4,H*4), 0, (W,H))\n",
    "# Kinv is the inverse of K\n",
    "Kinv = np.linalg.inv(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the match graph for all images\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(np.vstack(mm.getMatchGraph()[:]) > -1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the matches between image 0 and image 1\n",
    "plt.imshow(cv2.drawMatches(mm.images[0],mm.kpts[0],mm.images[1],mm.kpts[1],mm.matches[(0,1)],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS))\n",
    "plt.title('image 0 -> 1: # points %d'%(len(mm.matches[(0,1)])));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the matches between image 3 and image 7\n",
    "plt.imshow(cv2.drawMatches(mm.images[3],mm.kpts[3],mm.images[7],mm.kpts[7],mm.matches[(3,7)],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS))\n",
    "plt.title('image 3 -> 7: # points %d'%(len(mm.matches[(3,7)])));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of inliers between images 0, 1 and 2 and the rest of the images\n",
    "# in a multi-bar plot\n",
    "N = len(mm.images)\n",
    "plt.bar(np.arange(1,N)-0.11,[len(mm.matches[(0,i)]) for i in range(1,N)], label='image 0', width=0.33)\n",
    "plt.bar(np.arange(1,N)+0.22,[0]+[len(mm.matches[(1,i)]) for i in range(2,N)], label='image 1', width=0.33)\n",
    "plt.bar(np.arange(1,N)+0.55,[0,0]+[len(mm.matches[(2,i)]) for i in range(3,N)], label='image 2', width=0.33)\n",
    "plt.legend()\n",
    "plt.title('number of inliers');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the sharp drop in inlier matches as the views change the perspective. Only consecutive images have a high inlier count.\n",
    "\n",
    "However - we do want to work with a wide baseline. That's because very small shifts between viewpoints can approximate a Homography (\"flat\" planar) transformation, which is NOT what we want."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes your part!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Two-frame Structure-from-Motion\n",
    "\n",
    "1. Estimate essential matrix (with re-scaling)\n",
    "1. Decompose to find $[R|t]$\n",
    "1. Triangulate sparse 3D point cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the essential matrix - the linear method\n",
    "\n",
    "The epipolar constraint: $x_\\mathrm{R}^\\top E x_\\mathrm{L} = 0$ leads to the following $Ab=0$ system of equations: (Szeliski's eqn 11.33)\n",
    "$$\n",
    "\\begin{pmatrix}x_R & y_R & 1 \\end{pmatrix}\n",
    "\\begin{bmatrix}e_{00} & e_{01} & e_{02} \\\\ e_{10} & e_{11} & e_{12} \\\\ e_{20} & e_{21} & e_{22}\\end{bmatrix}\\begin{pmatrix}x_L\\\\y_L\\\\1 \\end{pmatrix} = 0\n",
    "\\\\\n",
    "\\begin{pmatrix}\n",
    "x_Re_{00} + y_Re_{10} + e_{20} &\n",
    "x_Re_{01} + y_Re_{11} + e_{21} &\n",
    "x_Re_{02} + y_Re_{12} + e_{22} \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x_L\\\\y_L\\\\1 \\end{pmatrix} = 0\n",
    "\\\\\n",
    "x_Lx_Re_{00} + x_Ly_Re_{10} + x_Le_{20} + \n",
    "y_Lx_Re_{01} + y_Ly_Re_{11} + y_Le_{21} +\n",
    "x_Re_{02} + y_Re_{12} + e_{22} \n",
    "= 0\n",
    "\\\\\n",
    "\\begin{pmatrix}\n",
    "\\cdots \\\\\n",
    "x_Lx_R & y_Lx_R & x_R &\n",
    "x_Ly_R & y_Ly_R & y_R &\n",
    "x_L & y_L & 1 \\\\\n",
    "\\cdots \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "e_{00} \\\\ e_{01} \\\\ e_{02} \\\\ e_{10} \\\\ e_{11} \\\\ e_{12} \\\\ e_{20} \\\\ e_{21} \\\\ e_{22} \\\\\n",
    "\\end{pmatrix}=0\n",
    "$$\n",
    "(where every row of A is essentially $x_\\mathrm{R}^\\top x_\\mathrm{L}$ ($3\\times3$) flattened, e.g. `(xR.T @ xL).ravel()`)\n",
    "$$\n",
    "\\begin{pmatrix}x_R \\\\ y_R \\\\ 1 \\end{pmatrix}\\begin{pmatrix}x_L & y_L & 1 \\end{pmatrix}=\n",
    "\\begin{bmatrix}x_Lx_R & y_Lx_R & x_R \\\\ x_Ly_R & y_Ly_R & y_R  \\\\ x_L & y_L & 1  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "However, remember we talked about the scaling problem, where $x*x$ is orders of magnitude larger than $x$ and $1$, therefore we should normalize the points to the $[-1,1]$ range before solving. Afterwards we can apply the inverse scaling to $E$ to negate this effect.\n",
    "\n",
    "Populate the matrix $A$ and solve for $E$ in the least squares sense: $\\hat{b} = \\mathop{\\arg\\min}_b|Ab|^2$, which means taking the SVD (`np.linalg.svd`) and using the last row of $V^\\top$, where the singular value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an aligned list of 2D image points from view 3 and 7\n",
    "mpts01o,mpts10o = mm.aligned2D(3, 7) # `o` is for \"original\"\n",
    "\n",
    "# Transform the points to normalized coordinates - meaning multiply on the left by the inverse of the camera matrix\n",
    "# use cv2.convertPointsToHomogeneous to convert the points to homogeneous coordinates\n",
    "# due to some weirdness in the way cv2 works, we need to use np.squeeze to remove the extra dimension\n",
    "# as well as .T (transpose) to get the correct shape (N,3) instead of (N,1,3)\n",
    "# the whole thing looks like this: np.matmul(Kinv, cv2.convertPointsToHomogeneous(pts).squeeze().T).T\n",
    "mpts01f,mpts10f = np.matmul(Kinv, cv2.convertPointsToHomogeneous(mpts01o).squeeze().T).T, np.matmul(Kinv, cv2.convertPointsToHomogeneous(mpts10o).squeeze().T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Essential Matrix according to the above equations\n",
    "# Populate A matrix and solve Ab=0 in LLSQ sense. Take E as last row of VT (last column of V)\n",
    "def calculateEssentialMatrix(ptsLeftHomog, ptsRightHomog):\n",
    "    A = np.zeros((ptsLeftHomog.shape[0], 9))\n",
    "    for row in range(A.shape[0]):\n",
    "        xL = ptsLeftHomog[row].reshape(1,3)\n",
    "        xR = ptsRightHomog[row].reshape(1,3)\n",
    "\n",
    "        A[row] = (xR.T @ xL).ravel()\n",
    "\n",
    "    _,_,V = np.linalg.svd(A)\n",
    "    E = V[-1].reshape(3,3)\n",
    "\n",
    "    return E"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a \"poor man's RANSAC\", randomly selecting 9 point-pairs and calculating $E$ from them, then counting the supporting \"inlier\" points by checking how far they are from their corresponding epilines.\n",
    "\n",
    "We will be using our homebrew RANSAC again further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANSAC is one of the most popular methods for finding the best model fit for a given set of data\n",
    "# In this case, we want to find the best Essential Matrix for the given set of 2D image points\n",
    "# We will use the following steps:\n",
    "# 1. Randomly select 9 points from the set of 2D image points\n",
    "# 2. Calculate the Essential Matrix for these 9 points with the above function (calculateEssentialMatrix)\n",
    "# 3. Find the inliers for this Essential Matrix\n",
    "# 4. Repeat steps 1-3 15000 times and keep the best model (the one with the most inliers)\n",
    "# 5. Recalculate the Essential Matrix for the inliers of the best model\n",
    "\n",
    "maxv = 0\n",
    "for i in range(15000):\n",
    "    # sample 9 points randomly from mpts01f and calculate `E_guess` (with calculateEssentialMatrix)\n",
    "    # then use `E_guess` to find the inliers by calculating the epipolar distance\n",
    "    # e.g. your epilines would be l0 = E_guess @ mpts01f.T  (this will be a 3xN matrix)\n",
    "    # then you can calculate the epipolar distance d for each point by doing\n",
    "    # e.g. [np.matmul(mpts10f[i], l) for i,l in enumerate(l0)]\n",
    "    # and then you can find the inliers by e.g. d < 0.0002\n",
    "    # recalculate the E matrix with the inliers by \"masking\" the points\n",
    "    # keep a \"score\" for the number of inliers and find the best model\n",
    "    # save the best model in the variable `E`\n",
    "\n",
    "    sample = np.random.choice(mpts01f.shape[0], 9, replace=False)\n",
    "    E_guess = calculateEssentialMatrix(mpts01f[sample], mpts10f[sample])\n",
    "\n",
    "    l0 = E_guess @ mpts01f.T\n",
    "    d = np.array([np.matmul(mpts10f[i], l) for i,l in enumerate(l0.T)])\n",
    "\n",
    "    inliers = d < 0.0002\n",
    "    error = np.sum(np.abs(d[inliers]))\n",
    "\n",
    "    if np.sum(inliers) > maxv:\n",
    "        maxv = np.sum(inliers)\n",
    "        E = calculateEssentialMatrix(mpts01f[inliers], mpts10f[inliers])\n",
    "\n",
    "        print(f\"{maxv} / {mpts01f.shape[0]}. error = {error}\")\n",
    "\n",
    "E, np.sum(inliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cv2.findEssentialMat to verify your result\n",
    "E_cv, mask = cv2.findEssentialMat(mpts01f[:,:2],mpts10f[:,:2],np.eye(3,3),method=cv2.RANSAC,prob=0.999,threshold=0.0002)\n",
    "E_cv, cv2.countNonZero(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your result is widely different from the cv2 result, check your work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epipolar Lines\n",
    "\n",
    "Let's inspect the epipolar lines arising from the essential matrix you've found. Verifying the epilines make sense is a great way to overall make sure your solution is good and matches your expectation (i.e the motion between cameras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the fundamental matrix from the essential matrix by using the camera matrix: F = Kinv.T * E * Kinv\n",
    "# you can optionally normalize the fundamental matrix by dividing it by its F[2,2] element\n",
    "# save it in the variable `F_fromE`\n",
    "\n",
    "F_fromE = np.matmul(Kinv.T, np.matmul(E, Kinv))\n",
    "F_fromE /= F_fromE[2,2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the epipolar lines, which is simply $l_\\mathrm{Left} = F \\tilde{x}_\\mathrm{Right}$, where $\\tilde{x}$ is the homogeneous augmented 2D point $x$. Each point in the right image is a line on the left image, w.l.o.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the epipolar lines from the F matrix, just like you've done before in the previous \n",
    "# section with the essential matrix.\n",
    "# use cv2.convertPointsToHomogeneous to convert pts_list to homogeneous coordinates\n",
    "def computeEpipolarLines(F, pts_list):\n",
    "    pts_list = cv2.convertPointsToHomogeneous(pts_list).squeeze().T\n",
    "    lines = F @ pts_list\n",
    "    return lines.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for your convenience, we've provided the code to draw the epipolar lines.\n",
    "# you can use it to verify your work and that the epipolar lines match what we got.\n",
    "lines_right_image = computeEpipolarLines(F_fromE, mpts01o) # left points -> lines in right image\n",
    "lines_left_image = computeEpipolarLines(F_fromE.T, mpts10o) # right points -> lines in left image\n",
    "\n",
    "# draw epipolar lines\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(images[3])\n",
    "step = 10\n",
    "for i in range(0, len(lines_left_image), step):\n",
    "    a,b,c = lines_left_image[i]\n",
    "    x0,y0 = 0, int(-c/b)\n",
    "    x1,y1 = W, int((-a*W-c)/b)\n",
    "    plt.plot([x0,x1],[y0,y1],color='r')\n",
    "    plt.scatter([mpts01o[i,0]],[mpts01o[i,1]],c='b')\n",
    "plt.axis('off')\n",
    "plt.ylim(H, 0)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(images[7])\n",
    "for i in range(0, len(lines_right_image), step):\n",
    "    a,b,c = lines_right_image[i]\n",
    "    x0,y0 = map(int, [0, -c/b])\n",
    "    x1,y1 = map(int, [W, -(a*W+c)/b])\n",
    "    plt.plot([x0,x1],[y0,y1],color='b')\n",
    "    plt.scatter([mpts10o[i,0]],[mpts10o[i,1]],c='r')\n",
    "# remove the axes\n",
    "plt.axis('off')\n",
    "# crop to just the image (no whitespace)\n",
    "plt.ylim(H, 0)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your epilines make sense, e.g. that they convrge in a point in the right side of the image but outside of the image - that's where the other camera would be. The \"right\" image will have the other camera on the left, and vice versa."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose $E$ to $[R|t]$\n",
    "Recall the essential matrix is composed: $E = [t]_\\times R$.\n",
    "\n",
    "The decomposition can be performed using SVD, e.g. $E = U\\Sigma V^\\top$, and set $t$ to be the last column of $\\pm U$, while\n",
    "$$\n",
    "\\begin{align}\n",
    "W &= \\begin{bmatrix}0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}  \\mathrm {,\\,\\,A\\,\\, 90^\\circ\\,\\, rotation}\\\\\n",
    "R'_1 &= UWV^\\top\\\\\n",
    "R'_2 &= UW^{-1}V^\\top\\\\\n",
    "R'_3 &= -UWV^\\top\\\\\n",
    "R'_4 &= -UW^{-1}V^\\top\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "But we keep only the 2 rotation matrices e.g. $R_1, R_2$ that have positive determinant.\n",
    "\n",
    "This results in 4 configurations: $(t,R_1)$,$(t,R_2)$,$(-t,R_1)$,$(-t,R_2)$. See your readings for the reason.\n",
    "\n",
    "To find the correct pair we should use the \"cheirality check\", which essentially means we triangulate 3D points and check they have positive Z coordinates and they are indeed in front of both cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take SVD of E\n",
    "# t is the last column of U\n",
    "# there are four possible rotations: R1 = UWV^T, R2 = UW^TV^T, R3 = -UWV^T, R4 = -UW^TV^T\n",
    "# but keep only the 2 rotations with positive determinant (use np.linalg.det)\n",
    "# keep the good rotations in a variable called Rs\n",
    "\n",
    "U, S, V = np.linalg.svd(E)\n",
    "t = U[:,2:]\n",
    "W = np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n",
    "\n",
    "R1 = np.matmul(U, np.matmul(W, V))\n",
    "R2 = np.matmul(U, np.matmul(W.T, V))\n",
    "R3 = np.matmul(-U, np.matmul(W, V))\n",
    "R4 = np.matmul(-U, np.matmul(W.T, V))\n",
    "\n",
    "Rs = [R1, R2, R3, R4]\n",
    "Rs = [R for R in Rs if np.linalg.det(R) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out your results\n",
    "Rs,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and you can cross-check your result with cv2.decomposeEssentialMat\n",
    "cv2.decomposeEssentialMat(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again if you got a vastly different result, check your work\n",
    "# but small changes in the domain of floating point errors are fine (e.g. 1e-3 range)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangulate 3D points\n",
    "Recall our work on triangulation once we have found the $R,t$ parameters:\n",
    "$$\n",
    "\\displaystyle\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\\lambda x^{(l)}\\\\\\lambda y^{(l)}\\\\\\lambda\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "f_x & 0 & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "r_1 & r_2 & r_3 & t_x\\\\\n",
    "r_4 & r_5 & r_6 & t_y\\\\\n",
    "r_7 & r_8 & r_9 & t_z\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}X\\\\Y\\\\Z\\\\1\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "p_{11} & p_{12} & p_{13} & p_{14} \\\\\n",
    "p_{21} & p_{22} & p_{23} & p_{24} \\\\\n",
    "p_{31} & p_{32} & p_{33} & p_{34} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}X\\\\Y\\\\Z\\\\1\\end{bmatrix}\n",
    "\\\\\n",
    "\\begin{bmatrix}\\lambda x^{(r)}\\\\\\lambda y^{(r)}\\\\\\lambda\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "f_x & 0 & c_x & 0\\\\\n",
    "0 & f_y & c_y & 0\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}X\\\\Y\\\\Z\\\\1\\end{bmatrix}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Expand\n",
    "$$\n",
    "\\begin{align}\n",
    "x^{(l)}p_{31}X + x^{(l)}p_{32}Y + x^{(l)}p_{33}Z + x^{(l)}p_{34} &= p_{11}X + p_{12}Y + p_{13}Z + p_{14}\\\\\n",
    "y^{(l)}p_{31}X + y^{(l)}p_{32}Y + y^{(l)}p_{33}Z + y^{(l)}p_{34} &= p_{21}X + p_{22}Y + p_{23}Z + p_{24}\\\\\n",
    "x^{(r)}Z &= f_x X + c_x Z \\\\\n",
    "y^{(r)}Z &= f_y Y + c_y Z\n",
    "\\end{align}\n",
    "$$\n",
    "Rearrange\n",
    "$$\n",
    "\\begin{align}\n",
    "(x^{(l)}p_{31} - p_{11})X + (x^{(l)}p_{32} - p_{12})Y + (x^{(l)}p_{33} - p_{13})Z + x^{(l)}p_{34} - p_{14} &= 0\\\\\n",
    "(y^{(l)}p_{31} - p_{21})X + (y^{(l)}p_{32} - p_{22})Y + (y^{(l)}p_{33} - p_{23})Z + y^{(l)}p_{34} - p_{24} &= 0\\\\\n",
    "-f_x X + 0Y + (x^{(r)} - c_x)Z &= 0 \\\\\n",
    "0X + -f_y Y + (y^{(r)} - c_y)Z &= 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Matrix form\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x^{(l)}p_{31} - p_{11} & x^{(l)}p_{32} - p_{12} & x^{(l)}p_{33} - p_{13} & x^{(l)}p_{34} - p_{14} \\\\\n",
    "y^{(l)}p_{31} - p_{21} & y^{(l)}p_{32} - p_{22} & y^{(l)}p_{33} - p_{23} & y^{(l)}p_{34} - p_{24} \\\\\n",
    "-f_x & 0 & x^{(r)} - c_x \\\\\n",
    "0 & -f_y & y^{(r)} - c_y\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}X\\\\Y\\\\Z\\\\1\\end{bmatrix}=0\n",
    "$$\n",
    "\n",
    "Now if we have 2 contributions to this system we can solve this linear system of equations $Ax=0$ in the constrained least squares sense (`np.linalg.svd`, take last row ot $V^\\top$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the triangulation routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that triangulates points given a \"right\" camera extrinsics (R,t)\n",
    "# assume that the \"left\" camera intrinsics are the identity matrix\n",
    "# first compute the projection matrix P = K[R|t] (use e.g. np.matmul)\n",
    "# for every point:\n",
    "#   populate the A matrix according to the above equations\n",
    "#   use np.linalg.svd to compute the SVD of A\n",
    "#   the 3D point is the last column of V (last row of V^T)\n",
    "#   normalize the homogeneous 3D point by dividing by its last element (homogeneous coordinate divide)\n",
    "# return the 3D points in a numpy array of shape (N,3)\n",
    "def triangulatePoints(pts2Dr, pts2Dl, R, t, K_):\n",
    "    Pl = np.matmul(K_, np.hstack((R, t)))\n",
    "    Pr = np.matmul(K_, np.hstack((np.eye(3), np.zeros((3,1)))))\n",
    "\n",
    "    pts3D = np.zeros((len(pts2Dl), 3))\n",
    "    for i in range(len(pts2Dl)):\n",
    "        pl = pts2Dl[i].reshape(2,1)\n",
    "        pr = pts2Dr[i].reshape(2,1)\n",
    "\n",
    "        # populate A\n",
    "        Al = np.matmul(pl, Pl[-1:]) - Pl[:2]\n",
    "        Ar = np.matmul(pr, Pr[-1:]) - Pr[:2]\n",
    "        A = np.vstack((Al, Ar))\n",
    "\n",
    "        # compute SVD of A\n",
    "        _,_,V = np.linalg.svd(A)\n",
    "        pt3D = V[-1]\n",
    "\n",
    "        # normalize\n",
    "        pts3D[i] = pt3D[:-1] / pt3D[-1]\n",
    "\n",
    "    return pts3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a decision about the 4 possible configurations (e.g. $(t,R_1)$,$(t,R_2)$,$(-t,R_1)$,$(-t,R_2)$) using the following criteria:\n",
    "1. Cheirality check: Count how many points are in front of the camera (positive +z coordinate)\n",
    "1. Reprojection check: Distance between original 2D point and 3D point reprojected back to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have two possible rotations and two possible translations (4 possible solutions)\n",
    "# we need to pick the best one. We can do this by checking the cheirality of the points\n",
    "# and the reprojection error. The best solution is the one with the most points in front\n",
    "# of the camera (Z > 0) and the smallest reprojection error.\n",
    "# write a loop that tries all 4 possible solutions and pick the best one.\n",
    "# the solutions are: (Rs[0],t), (Rs[0],-t), (Rs[1],t), (Rs[1],-t)\n",
    "# for each solution:\n",
    "#   triangulate the points with triangulatePoints you wrote above\n",
    "#   reproject the points back to the image domain using cv2.projectPoints\n",
    "#   compute the cheirality (number of points with Z > 0) and the reprojection error\n",
    "#   compute a score as the ratio of cheirality to reprojection error\n",
    "#   print out the cheirality, reprojection error and score for each solution\n",
    "#   keep the best score and the corresponding rotation and translation\n",
    "# when using cv2.projectPoints, if you're projecting on the left image, use R = np.eye(3), t = np.zeros((3,1))\n",
    "# if you're projecting on the right image, use R = Rs[i], t = t\n",
    "# to calculate the reprojection error, use np.linalg.norm to compute the norm of the difference between the\n",
    "# reprojected points (from cv2.projectPoints) and the original points (e.g. mpts01o for the left image)\n",
    "maxv = 0\n",
    "R_best = t_best = None\n",
    "for rot,tra in [(Rs[0],t), (Rs[0],-t), (Rs[1],t), (Rs[1],-t)]:\n",
    "    X = triangulatePoints(mpts01o, mpts10o, rot, tra, K)\n",
    "\n",
    "    reproj_l, _ = cv2.projectPoints(X, np.eye(3), np.zeros((3,1)), K, None)\n",
    "    reproj_r, _ = cv2.projectPoints(X, rot, tra, K, None)\n",
    "    \n",
    "    cheirality = np.sum(X[:,2] > 0)\n",
    "    \n",
    "    reproj_error = np.linalg.norm(reproj_l.squeeze() - mpts01o) + np.linalg.norm(reproj_r.squeeze() - mpts10o)\n",
    "    \n",
    "    score = cheirality / reproj_error\n",
    "    \n",
    "    print('cheirality: {}, reproj_error: {}, score: {}'.format(cheirality, reproj_error, score))\n",
    "    \n",
    "    if score > maxv:\n",
    "        maxv = score\n",
    "        R_best = rot\n",
    "        t_best = tra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reprojection error above should be < 10. If that's not the case it's likely the $E$ matrix isn't right - try finding it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_best, t_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify your result vs cv2.recoverPose\n",
    "_,R_cv,t_cv,_,_ = cv2.recoverPose(E, mpts01o, mpts10o, K, distanceThresh=3.0)\n",
    "R_cv,t_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as always - if your result is vastly different, check your work.\n",
    "# you should expect to get a result that is very very close to the cv2.recoverPose result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should start building our 3D map, saying for every 3D point which 2D views and points support it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triangulate points using the best rotation and translation\n",
    "pts3d = triangulatePoints(mpts01o, mpts10o, R_best, t_best, K)\n",
    "\n",
    "# since this has to do with the match graph - i provide the code for this part\n",
    "\n",
    "# reproject points back to image domain\n",
    "projPts2dLeft,_  = cv2.projectPoints(pts3d, (0,0,0), (0,0,0), K, None)\n",
    "projPts2dRight,_ = cv2.projectPoints(pts3d, R_best, t_best, K, None)\n",
    "\n",
    "# only keep points that have a small reprojection error (less than 5 pixels)\n",
    "mask_reproj = np.all(np.dstack([\n",
    "    np.linalg.norm(projPts2dRight.squeeze() - mpts10o, axis=1) < 5, \n",
    "    np.linalg.norm(projPts2dLeft.squeeze() - mpts01o, axis=1) < 5]), axis=2).squeeze()\n",
    "\n",
    "# update the 3D point map, and visibility graph with the visibility of each point.\n",
    "# currently the 3D points are visible from only two images: 3 and 7\n",
    "mm.addNewPoints3D(pts3d, 3, 7, mask_reproj)\n",
    "mm.poses[3] = np.hstack([np.eye(3), np.zeros((3,1))])\n",
    "mm.poses[7] = np.hstack([R_best, t_best])\n",
    "\n",
    "# repoject the points back to the image domain again after filtering for visualization\n",
    "projPts2dLeft,_  = cv2.projectPoints(mm.map_3d, mm.R(3), mm.t(3), K, None)\n",
    "projPts2dRight,_ = cv2.projectPoints(mm.map_3d, mm.R(7), mm.t(7), K, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the visibility matrix\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow((mm.point3d_camera_visibility > 0)[:,:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the points reprojected alongside the originals, any strong deviation here will suggest a bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again here's provided code to visualize the 3D points and the reprojection error\n",
    "# use this to verify that your code is working correctly\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(images[3])\n",
    "plt.scatter(mpts01o[:,0],mpts01o[:,1],label='Original 2D', s=20)\n",
    "plt.scatter(projPts2dLeft[:,0,0],projPts2dLeft[:,0,1],label='Reprojected 3D',s=3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(images[7])\n",
    "plt.scatter(mpts10o[:,0],mpts10o[:,1],label='Original 2D', s=20)\n",
    "plt.scatter(projPts2dRight[:,0,0],projPts2dRight[:,0,1],label='Reprojected 3D',s=3)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pts3d[:,0],pts3d[:,2])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Z')\n",
    "plt.title('X-Z plane (view from above)');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the wall and the fountain basin coming out of it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Incremental SfM\n",
    "Add another camera to your reconstructed scene: \n",
    "1. Find matching 2D-3D points\n",
    "1. Find camera pose with linear pose estimation\n",
    "1. Triangulate additional 3D points with the new camera pose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D-3D correspondences\n",
    "Let's add the view #5. To find camera pose we need to get 2D-3D correspondences.\n",
    "To get correspondences we go back to the original matching (Image 3 $\\leftrightarrow$ Image 7), and select the 2D points in Image 3 (which created 3D points with Image 7) intersected with 2D points in Image 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get an aligned set of points from the graph and visualize them\n",
    "selected_3_kpts, selected_5_kpts = mm.alignedKptsTo3DMap(3, 5)\n",
    "\n",
    "img_5 = cv2.drawKeypoints(images[5], selected_5_kpts, None, color=(0, 255, 0), flags=0)\n",
    "img_3 = cv2.drawKeypoints(images[3], selected_3_kpts, None, color=(0, 255, 0), flags=0)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_5)\n",
    "plt.title('Image 5')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img_3)\n",
    "plt.title('Image 3')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seeing the two views and their points make sense is an important step for verification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera pose - the linear method\n",
    "Remember, as per usual we start from\n",
    "$$\n",
    "\\lambda\\begin{bmatrix}x\\\\y\\\\1\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}f_x & 0 & c_x\\\\0 & f_y & c_y \\\\0 & 0 & 1\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "r_1 & r_2 & r_3 & t_x\\\\\n",
    "r_4 & r_5 & r_6 & t_y\\\\\n",
    "r_7 & r_8 & r_9 & t_z\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}X\\\\Y\\\\Z\\\\1\\end{bmatrix}\n",
    "$$\n",
    "Multiply on left with $K^{-1}$ (essentially use normalized coordinates) and rearrange (after finding $\\lambda$):\n",
    "$$\n",
    "\\displaystyle\n",
    "\\begin{bmatrix}\n",
    "x'(r_7X+r_8Y+r_9Z+t_z)\\\\\n",
    "y'(r_7X+r_8Y+r_9Z+t_z)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "r_1 & r_2 & r_3 & t_x\\\\\n",
    "r_4 & r_5 & r_6 & t_y\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}X\\\\Y\\\\Z\\\\1\\end{bmatrix}\n",
    "$$\n",
    "Which leads, with further rearrangement, to an $Ab=0$ homogeneous system of equations to find $R,t$:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\cdots \\\\\n",
    "X_i & Y_i & Z_i & 1 & 0    & 0    & 0    & 0  & -x'_iX_i & -x'_iY_i & -x'_iZ_i & -x'_i  \\\\\n",
    "0    & 0    & 0    & 0  & X_i & Y_i & Z_i & 1 & -y'_iX_i & -y'_iY_i & -y'_iZ_i & -y'_i \\\\\n",
    "\\cdots \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "r_1 \\\\ r_2 \\\\ r_3 \\\\ t_x \\\\ r_4 \\\\ r_5 \\\\ r_6 \\\\ t_y \\\\ r_7 \\\\ r_8 \\\\ r_9 \\\\ t_z\n",
    "\\end{bmatrix}\n",
    "= 0\n",
    "$$\n",
    "Which we can solve by solving the \"minimal direction problem\" ($\\hat{b} = \\mathop{\\arg\\min}_b|Ab|^2 \\,\\,\\,\\,\\, \\mathrm{s.t.} \\,\\,\\, |b|=1\n",
    "$), which essentially means we take the SVD: $A=U\\Sigma V^\\top$, and take as the solution the last row of $V^\\top$, and (according to this formulation) simply reshape it $3\\times4$ to obtain P.\n",
    "\n",
    "However, the calculated matrix $P$ can take on an arbitrary scale, so the $R_{3\\times3}$ matrix may need some conditioning to become a true rotation (orthonormal), effectively removing the scaling factor. Therefore we take the SVD and omit the scaling matrix $\\Sigma$:\n",
    "$$\n",
    "\\begin{align}\n",
    "R &= U\\Sigma V^\\top\\\\\n",
    "\\hat{R} &= UV^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "When we do that we need to impose the same rescaling on the translation: $\\hat{t} = t\\cdot\\sum \\hat{R}/R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that takes in a set of 2D-3D correspondences and returns the camera pose\n",
    "# this function should use the DLT (direct linear transform) algorithm we wrote above\n",
    "# populate the A matrix with 2 entries from each correspondence according to the equations above\n",
    "# then use SVD to solve for the camera pose P (3x4 matrix). \n",
    "# the pose would be the last column of the V matrix from the decomposition of A (i.e. V[:,-1]).\n",
    "# if the Rotation component (left 3x3 submatrix) of the 3x4 pose matrix has a negative determinant, \n",
    "# multiply it by -1\n",
    "# apply the method above to make sure the rotation is orthonormal. take its SVD and reassemble\n",
    "# the rotation matrix from the U and V matrices (e.g. R_conditioned = U @ V^T)\n",
    "# scale the translation (the 3x1 right submatrix of P) by the sum (R_conditioned / R)\n",
    "# return R_conditioned, t_conditioned\n",
    "def calculateCameraPose(corresp2D3D_):\n",
    "    A = np.zeros((2*len(corresp2D3D_), 12))\n",
    "    for i, (x, X) in enumerate(corresp2D3D_):\n",
    "        A[2*i]   = [X[0], X[1], X[2], 1, 0, 0, 0, 0, -x[0]*X[0], -x[0]*X[1], -x[0]*X[2], -x[0]]\n",
    "        A[2*i+1] = [0, 0, 0, 0, X[0], X[1], X[2], 1, -x[1]*X[0], -x[1]*X[1], -x[1]*X[2], -x[1]]\n",
    "\n",
    "    _,_,V = np.linalg.svd(A)\n",
    "    P = V[-1].T.reshape(3,4)\n",
    "    \n",
    "    R = P[:,:3]\n",
    "    t = P[:,3:]\n",
    "    if np.linalg.det(R) < 0:\n",
    "        R = -R\n",
    "    \n",
    "    U,_,V = np.linalg.svd(R)\n",
    "    R_cond = U @ V.T\n",
    "    t_cond = t / np.sum(R_cond / R)\n",
    "\n",
    "    return R_cond, t_cond"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use a \"RANSAC\" type method to find a robust solution while weeding out the outliers. We take a lax margin for inlier inclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 2D-3D correspondences from the graph (as well as \"backmapping\" so we know how to map back \n",
    "# to the originating 2D points).\n",
    "correspond2D3D, backmapping = mm.aligned2D3D(5)\n",
    "correspond2D3D = correspond2D3D.astype(np.float32)\n",
    "# the correspond2D3D array is a 3D array of shape (N,2,3) where N is the number of 2D-3D correspondences\n",
    "# the first index is the correspondence pair index\n",
    "# the second index is 0 for the 2D point (homogeneous, so it's 1x3) and 1 for the 3D point (1x3)\n",
    "# for example correspond2D3D[0,0] is the 2D point and correspond2D3D[0,1] is the 3D point\n",
    "\n",
    "# here's another opportunity to use the RANSAC algorithm to find the best camera pose\n",
    "# use the function you wrote above to calculate the camera pose for a random sample of 6 correspondences\n",
    "# then find the inliers (2D points that are within 25 pixels of the reprojected 3D points)\n",
    "# using cv2.projectPoints, and np.linalg.norm (and .squeeze() as needed).\n",
    "# the best camera pose is the one with the most inliers (use the maxv variable below to keep track of this)\n",
    "# save the final camera pose in R_final and t_final\n",
    "maxv = 0\n",
    "for i in range(25_000):\n",
    "    sample = np.random.choice(len(correspond2D3D), 6, replace=False)\n",
    "    R,t = calculateCameraPose(correspond2D3D[sample])\n",
    "\n",
    "    projPts2d,_ = cv2.projectPoints(correspond2D3D[:,1:], R, t, K, None)\n",
    "\n",
    "    # print(projPts2d.squeeze())\n",
    "\n",
    "    dists = np.linalg.norm(correspond2D3D[:,0,:-1] - projPts2d.squeeze(), axis=1)\n",
    "    inliers = dists < 25\n",
    "\n",
    "    if np.sum(inliers) > maxv:\n",
    "        maxv = np.sum(inliers)\n",
    "        R_final = R\n",
    "        t_final = t\n",
    "\n",
    "        print(maxv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pritnt your results\n",
    "print(R_final,t_final.T,maxv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again compare your results to OpenCV's solvePnPRansac function\n",
    "# you rsults should be very similar, but not exactly the same. a small difference is expected\n",
    "# you very well can run the RANSAC loop above multiple times and get other reuslts\n",
    "_,r_cv,t_cv,inliers_cv = cv2.solvePnPRansac(correspond2D3D[:,1], correspond2D3D[:,0,:2], K, None)\n",
    "print(cv2.Rodrigues(r_cv)[0], t_cv.T, len(inliers_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the process above finds > 30% of the points as inliers. If it doesn't - run it again, it's a game of chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a visualization of the 2D-3D correspondences and the reprojected 3D points\n",
    "# the red points are the 2D points and the blue points are the reprojected 3D points\n",
    "# the reprojected 3D points should be close to the 2D points - but we gave it a good margin of 25 pixels\n",
    "# so we expect some of the blue points to be outside the red points.\n",
    "# outliars are points that are very far from the reprojected 3D points\n",
    "projPts2d2,_ = cv2.projectPoints(correspond2D3D[:,1].T, R_final,t_final, K, None)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(images[5])\n",
    "plt.scatter(correspond2D3D[:,0,0],correspond2D3D[:,0,1],label='2D Image', c='r')\n",
    "plt.scatter(projPts2d2[:,0,0],projPts2d2[:,0,1],label='Reprojected 3D',c='b',s=5)\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty bad doesn't it!\n",
    "\n",
    "We can do much better with the following step...\n",
    "\n",
    "Luckily (here is the power of non-linear optimization), if the initial solution is even remotely close (such as the case above), we can make pretty big steps towards a far better solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Camera Pose Non-Linear Optimization\n",
    "\n",
    "This time we complement with a non-linear least squares optimization, with a huber loss to help further with outliers, to minimize the reprojection loss: \n",
    "$$\n",
    "\\displaystyle\n",
    "\\hat{P} = \\mathop{\\arg\\min}_{P} \\sum_i \\Vert \\mathrm{Proj}(P,X_i^{\\mathrm{3D}}) - x_i^{\\mathrm{2D}} \\Vert\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that calculates the residuals (error) between 2D and reprojected-3D (also 2D) points given \n",
    "# the rotation and translation vectors (flattened into a 1D array, see below)\n",
    "# use the cv2.projectPoints function to reproject the 3D points (correspond2D3D[:,1]) to 2D\n",
    "# return the residuals (just a subtraction `p2D - pReproj2D`) as a 1D array (use the ravel() function)\n",
    "# the original 2D points are in correspond2D3D[:,0,:2]\n",
    "def calcResiduals(Rt):\n",
    "    pReproj2D,_ = cv2.projectPoints(correspond2D3D[:,1].astype(np.float32), Rt[:3], Rt[3:], K, None)\n",
    "    p2D = correspond2D3D[:,0,:2]\n",
    "\n",
    "    return (p2D - pReproj2D.squeeze()).ravel()\n",
    "    \n",
    "# use scipy least squares to refine the pose estimate (scipy.optimize.least_squares)\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html\n",
    "# however this time we will use a robus method (Huber loss function, e.g. loss='huber') to deal with outliers, as well as use\n",
    "# an iterative solver which works with the residuals.\n",
    "# the parameters to optimize are the rotation vector and the translation vector - in rodrigues form (cv2.Rodrigues)\n",
    "# stack them together (hstack) and use the ravel() function to convert to a 1D array, this will be the input to the\n",
    "# calcResiduals function you defined above.\n",
    "\n",
    "res = scipy.optimize.least_squares(calcResiduals, np.hstack((cv2.Rodrigues(R_final)[0].ravel(), t_final.ravel())), loss='huber', method='trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the solution from the least squares function's result\n",
    "R2 = cv2.Rodrigues(res.x[:3])[0]\n",
    "t2 = res.x[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our optimized pose\n",
    "R2, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare again to the OpenCV solution (r_cv, t_cv) and see that now they are virtually the same\n",
    "# the difference should now be very very small.\n",
    "cv2.Rodrigues(r_cv)[0], t_cv.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the new projected points to verify the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize the results again to see how well the reprojected 3D points match the 2D points\n",
    "projPts2d2,_ = cv2.projectPoints(correspond2D3D[:,1].T, R2, t2, K, None)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(mm.images[5])\n",
    "plt.scatter(correspond2D3D[:,0,0],correspond2D3D[:,0,1],label='2D Image', c='r', s=40)\n",
    "plt.scatter(projPts2d2[:,0,0],projPts2d2[:,0,1],label='Reprojected 3D', c='b', s=20)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is code provided to you to add the camera pose to the map.\n",
    "\n",
    "# add the camera pose to the map\n",
    "mm.poses[5] = np.hstack([R2,t2[np.newaxis].T])\n",
    "\n",
    "# add this view to the visibility graph match_graph_3d[5], where the reprojection \n",
    "# error is small (3d points we are confident are viewable from camera 5)\n",
    "projPts2d2,_ = cv2.projectPoints(correspond2D3D[:,1].T, mm.R(5), mm.t(5), K, None)\n",
    "for i in range(len(projPts2d2)):\n",
    "    if np.linalg.norm(np.squeeze(projPts2d2[i]) - correspond2D3D[i,0,:2]) < 5:\n",
    "        mm.point3d_camera_visibility[5, backmapping[i][0]] = backmapping[i][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the 3D point visibility map. Now we've added camera 5, it overlaps with some of the points, but some were thrown out because they're outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow((mm.point3d_camera_visibility > 0)[:,:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangulate more points\n",
    "\n",
    "Now that we found the pose for camera 5, let's add more 3D points to the reconstruction that come from the pairs: 3-5, and 5-7 (but don't already exist in the current 3-5-7 map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get aligned 2D points from 3,5 views that are not in the map\n",
    "# the backidxL and backidxR are the indices of the 2D points in the original keypoint arrays in the map\n",
    "pt2dl, pt2dr, backidxL, backidxR = mm.aligned2DNotInMap(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use cv2.triangulatePoints to triangulate the 2D points.\n",
    "# you will need to convert the 2D points to normalized coordinates before triangulation\n",
    "# that is, multiply on the left by the inverse of the camera matrix Kinv: \n",
    "#   np.matmul(Kinv, cv2.convertPointsToHomogeneous(pt2d).squeeze().T).T\n",
    "# the output of cv2.triangulatePoints is in homogeneous coordinates, so you will need to convert it to\n",
    "# euclidean coordinates using cv2.convertPointsFromHomogeneous.\n",
    "# make sure your final output is a 3xN array (N is the number of points)\n",
    "\n",
    "# convert to normalized coordinates\n",
    "ptN2dl = np.matmul(Kinv, cv2.convertPointsToHomogeneous(pt2dl).squeeze().T)\n",
    "ptN2dr = np.matmul(Kinv, cv2.convertPointsToHomogeneous(pt2dr).squeeze().T)\n",
    "\n",
    "# triangulate points\n",
    "Pl = np.hstack((mm.R(3), mm.t(3).reshape(3,1)))\n",
    "Pr = np.hstack((mm.R(5), mm.t(5).reshape(3,1)))\n",
    "\n",
    "pt3d = cv2.triangulatePoints(Pl, Pr, ptN2dl[:2], ptN2dr[:2]).T\n",
    "\n",
    "# convert to Euclidean coordinates\n",
    "pt3d = cv2.convertPointsFromHomogeneous(pt3d).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is some code to visualize the results\n",
    "# the blue points are the reprojected 3D points triangulated from the 2D\n",
    "# the red points are the original image 2D points\n",
    "projPts2d2,_ = cv2.projectPoints(pt3d.T, mm.R(5), mm.t(5), K, None)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(mm.images[5])\n",
    "plt.scatter(pt2dr[:,0],pt2dr[:,1],label='2D Image', c='r', s=40)\n",
    "plt.scatter(projPts2d2[:,0,0],projPts2d2[:,0,1],label='Reprojected 3D', c='b', s=20)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the new 3D points to the map\n",
    "M, N = mm.map_3d.shape[0], pt3d.shape[0]\n",
    "mm.point3d_camera_visibility[3, M:M+N] = backidxL\n",
    "mm.point3d_camera_visibility[5, M:M+N] = backidxR\n",
    "mm.map_3d = np.vstack([mm.map_3d, pt3d.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the map\n",
    "plt.scatter(mm.map_3d[:,0],mm.map_3d[:,2])\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(0.5,2.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Z')\n",
    "plt.title('X-Z plane (view from above)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the visibility graph\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow((mm.point3d_camera_visibility > 0)[:,:400])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Bundle Adjustment\n",
    "\n",
    "Write a bundle adjuster to optimize the reconstruction in Q.2: Find the optimal camera intrinsics, extrinsics (camera pose) and 3D points, such that the reprojection error (where there is visibility, guarded by $w_{ij}$) is minimal:\n",
    "$$\n",
    "\\hat{X}, \\hat{C}, \\hat{K} = \\mathop{\\arg\\min}_{X,C,K} \\sum_j^M \\sum_i^N w_{ij} \\left\\Vert \\mathrm{Proj}(X_i^\\mathrm{(3D)},C_j,K) - x_i^\\mathrm{(2D)} \\right\\Vert^2\n",
    "$$\n",
    "\n",
    "Assume:\n",
    "1. All cameras have the same K matrix\n",
    "2. Pixels are square ($f_x = f_y$)\n",
    "3. There is no skew ($K_{0,1} = 0$)\n",
    "\n",
    "Pack all the parameters for this reconstruction into a single (very long) vector like so:\n",
    "$$\n",
    "\\left[R^0_{1\\times3}, R^1_{1\\times3}, R^2_{1\\times3}, t^0_{1\\times3}, t^1_{1\\times3}, t^2_{1\\times3}, f, c_x, c_y, p^{0}_{\\mathrm{3D}},\\dots,p^{N}_{\\mathrm{3D}}\\right]\n",
    "$$\n",
    "Rotation matrices $3\\times3$ should be converted to Rodrigues formula $1\\times3$ (`cv2.Rodrigues(...)[0]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_3d, mpts2DForViews, visibility = mm.alignedMapTo2DAndVisibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vector of parameters for initialization\n",
    "# the number of parameters is: \n",
    "#   3 for each camera rotation, \n",
    "#   3 for each camera translation, \n",
    "#   3 for the intrinsics\n",
    "#   and 3 for each 3D point\n",
    "n_cams = 3\n",
    "cam_ids = [3,5,7]\n",
    "n_pts = mm.map_3d.shape[0]\n",
    "params_size = n_cams * 6 + n_pts * 3 + 3\n",
    "\n",
    "# the params vector. you will need to fill this in with the correct values\n",
    "x0 = np.zeros((params_size,), np.float32) \n",
    "\n",
    "# initialize the camera rotations and translations (6 params per cam) starting at 0\n",
    "# intrinsics start n_cams * 6, 3 params total\n",
    "# 3d points start at K_idx + 3, 3 params per point\n",
    "\n",
    "rots = [cv2.Rodrigues(mm.R(cam_id))[0] for cam_id in cam_ids]\n",
    "tras = [mm.t(cam_id) for cam_id in cam_ids]\n",
    "\n",
    "# fill in the params vector\n",
    "# for example, the first camera rotation (1x3 vector) is x0[0:3]. the j'th camera rotation is x0[j*3:j*3+3]\n",
    "# get the rotation from mm.R(camId) and translation (1x3 again) from mm.t(camId)\n",
    "# the intrinsics are K[0,0], K[0,2], K[1,2], and they go into x0[K_idx+0], x0[K_idx+1], x0[K_idx+2], \n",
    "# respectively where K_idx is the index of the first intrinsics parameter (e.g. after the cameras)\n",
    "# finally add the 3D points to the params vector as well\n",
    "\n",
    "# camera rotations and translations\n",
    "for i in range(n_cams):\n",
    "    x0[i*3:i*3+3] = rots[i].squeeze()\n",
    "    x0[i*3+3:i*3+6] = tras[i].squeeze()\n",
    "\n",
    "# intrinsics\n",
    "K_idx = n_cams * 6\n",
    "x0[K_idx+0] = K[0,0]\n",
    "x0[K_idx+1] = K[0,2]\n",
    "x0[K_idx+2] = K[1,2]\n",
    "\n",
    "# 3D points\n",
    "x0[K_idx+3:] = mm.map_3d.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the residuals vector, e.g. $\\left[\\dots,r_{ji}^x,r_{ji}^y,\\dots\\right]$ where $r_{ji} = \\left(\\mathrm{Proj}(P^{\\mathrm{3D}}_i,C_j,K)-p^{\\mathrm{2D}}_i\\right)$, unpacking the parameters for $P^{\\mathrm{3D}}_i,C_j,K$ from the `params` vector, and taking the 2D point $p^{\\mathrm{2D}}_i$ from the `pts2d` argument. The shape of the residuals vector is $1\\times2N$, where $N$ is the number of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be called by the optimizer to calculate the reprojection error (residuals)\n",
    "# the \"params\" vector contains the camera poses and 3D points flattened into a single vector\n",
    "# the \"n_cams\" and \"n_pts\" variables are the number of cameras and 3D points in the map\n",
    "# the \"cam_ids\" are camera ids (e.g. [3,5,7]), so we can iterate [0]->3, [1]->5, [2]->7\n",
    "# the \"pts2d\" are image 2D points that we want to compare to the reprojection (some of them are \"empty\")\n",
    "# the \"visibility\" is a matrix that indicates which 3D points are visible from which camera\n",
    "# the optimization cannot change \"pts2d\" and \"visibility\", these are our \"truth\"\n",
    "# the \"show_debug\" variable is used to visualize the reprojection error\n",
    "# the function returns a residual vector of the reprojection error (`a - b`) for each 2D point\n",
    "def calcResidualsBA(params, n_cams, n_pts, cam_ids, pts2d, visibility, show_debug=False):\n",
    "    # unpack the camera poses and 3D points from the \"params\" vector\n",
    "    # params = [R1 (1x3), t1 (1x3), ..., Rn (1x3), tn (1x3), f, cx, cy, X1, Y1, Z1, ..., Xn, Yn, Zn]\n",
    "    # a total of 6*n_cams + 3 + 3*n_pts parameters\n",
    "    # the camera poses are stored as Rodrigues vectors (3 parameters) and translations (3 parameters)\n",
    "    # the intrinsics are stored as focal length (1 parameter), and 2 principal point parameters\n",
    "    # the 3D points are stored as X,Y,Z coordinates (3 parameters each)\n",
    "    # extract Rs, ts, f, cx, cy, pts3d from the vector using the scheme above\n",
    "    # build intrinsics K from f, cx, cy\n",
    "    # then, for each camera (Rs[i],ts[i]) project the 3D points into the image plane with cv2.projectPoints\n",
    "    # for every projected point - check if it's visible in the image (visibility[cam_ids[i]][j] == 1)\n",
    "    # if it's visible, calculate the reprojection error (a - b) and append it to the \"resid\" vector\n",
    "    # if it's not visible, append [0,0] to the \"resid\" vector\n",
    "    # finally, return the \"resid\" vector stacked into a single vector (np.hstack)\n",
    "\n",
    "    # unpacking rotation and translation\n",
    "\n",
    "    Rs = []\n",
    "    ts = []\n",
    "\n",
    "    for i in range(n_cams):\n",
    "        Rs.append(cv2.Rodrigues(params[i*3:i*3+3])[0])\n",
    "        ts.append(params[i*3+3:i*3+6])\n",
    "\n",
    "    # unpacking intrinsics\n",
    "\n",
    "    f = params[n_cams*6]\n",
    "    cx = params[n_cams*6+1]\n",
    "    cy = params[n_cams*6+2]\n",
    "\n",
    "    K = np.array([[f, 0, cx], [0, f, cy], [0, 0, 1]])\n",
    "\n",
    "    # unpacking 3D points\n",
    "\n",
    "    pts3d = params[n_cams*6+3:].reshape(-1,3)\n",
    "\n",
    "    # projecting 3D points into image plane\n",
    "\n",
    "    resid = []\n",
    "\n",
    "    for i in range(n_cams):\n",
    "        proj_pts, _ = cv2.projectPoints(pts3d, Rs[i], ts[i], K.astype(np.float64), None)\n",
    "        proj_pts = proj_pts.squeeze()\n",
    "\n",
    "        for j in range(n_pts):\n",
    "            if visibility[cam_ids[i]][j] == 1:\n",
    "                resid.append(proj_pts[j] - pts2d[cam_ids[i]][j])\n",
    "            else:\n",
    "                resid.append([0,0])\n",
    "\n",
    "    resid = np.hstack(resid)\n",
    "\n",
    "    return resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This builds the sparsity matrix of the jacobian (partial derivative matrix), which greatly increases the speed of optimization. We let the optimizer calculate the jacobian by itself numerically (by adding small $\\Delta$'s to the parameters), which is a costly operation, therefore we guide it by saying what elements of the jacobian matrix will always be 0 and never have to be calculated. \n",
    "\n",
    "For each 2D point residual, many parameters for optimization in the system are irrelevant, and in fact a 2D residual is derived from just a handful of parameters: $K,C_j,P^{\\mathrm{3D}}_i$, therefore the jacobian is very sparse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# this function creates a sparse matrix that indicates which parameters affect which residuals\n",
    "# this is used by the optimizer to speed up the calculation\n",
    "# the \"n_cams\" and \"n_pts\" variables are the number of cameras and 3D points in the map\n",
    "# the \"cam_ids\" are camera ids (e.g. [3,5,7]), so we can iterate [0]->3, [1]->5, [2]->7\n",
    "# the \"visibility\" is a matrix that indicates which 3D points are visible from which camera,\n",
    "#   this is used to determine which residuals are affected by which parameters.\n",
    "# total number of potential residuals (some will be zero) = n_cams * n_pts * 2\n",
    "# total number of parameters = n_cams * 6 + n_pts * 3 + 3\n",
    "\n",
    "# so build a sparse matrix (`lil_matrix`) A of size (n_cams * n_pts * 2) x (n_cams * 6 + n_pts * 3 + 3)\n",
    "# in the residual vector, the camera parameters take up the first 6*n_cams parameters\n",
    "# the intrinsic parameters take up the next 3 parameters, starting at 6*n_cams\n",
    "# the 3D points take up the next 3*n_pts parameters, starting at 6*n_cams + 3.\n",
    "# to figure out where in the residual vector each parameter affects, use the following scheme:\n",
    "#   each residual (reprojection) is effected by the camera pose (R,t) that it was projected from\n",
    "#   each residual is also effected by the 3D point that was projected\n",
    "#   each residual is also effected by the intrinsic parameters (f, cx, cy)\n",
    "# the poses, 3d points and intrinsics are stored in the \"params\" vector in the order that we've\n",
    "#   described above, so you can use the same indexing scheme to figure out where each parameter.\n",
    "# \n",
    "# for example, a point `i`` in camera `j`` will affect sparsity matrix at \n",
    "#   rows: `2*i + j*n_pts*2` and `2*i + j*n_pts*2 + 1`\n",
    "#   columns: `j*3` -> `j*3 + 3` (the 3 parameters of the camera rotation)\n",
    "#            `ts_ids + j*3` -> `ts_ids + j*3 + 3` (the 3 parameters of the camera translation)\n",
    "#            `K_idx` -> `K_idx + 3` (the 3 parameters of the intrinsics)\n",
    "#            `pts3d_idx + i*3` -> `pts3d_idx + i*3 + 3` (the 3 parameters of the 3D point)\n",
    "#\n",
    "# sparse non-linear optimization problems are not easy to set up. so pay attention to the details.\n",
    "def bundle_adjustment_sparsity(n_cams, n_pts, cam_ids, visibility):\n",
    "    A = lil_matrix((n_cams * n_pts * 2, n_cams * 6 + n_pts * 3 + 3), dtype=np.float32)\n",
    "\n",
    "    for i in range(n_pts):\n",
    "        for j in range(n_cams):\n",
    "            if visibility[cam_ids[j]][i] == 1:\n",
    "                # rotation\n",
    "                A[2*i + j*n_pts*2    : 2*i + j*n_pts*2 + 2,\n",
    "                  j*3                : j*3 + 3            ] = 1\n",
    "                \n",
    "                # translation\n",
    "                A[2*i + j*n_pts*2    : 2*i + j*n_pts*2 + 2,\n",
    "                  n_cams*3 + j*3     : n_cams*3 + j*3 + 3 ] = 1\n",
    "                \n",
    "                # intrinsics\n",
    "                A[2*i + j*n_pts*2    : 2*i + j*n_pts*2 + 2,\n",
    "                  6*n_cams           : 6*n_cams + 3       ] = 1\n",
    "                \n",
    "                # points\n",
    "                A[2*i + j*n_pts*2    : 2*i + j*n_pts*2 + 2,\n",
    "                  6*n_cams + i*3 + 3 : 6*n_cams + i*3 + 6 ] = 1\n",
    "                \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = bundle_adjustment_sparsity(3, map_3d.shape[0], [3, 5, 7], visibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the sparsity of the jacobian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(A,markersize=0.03);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters for optimization should be bound, and not be allowed to get extreme values. For example the focal length $f$ as well as $c_x,c_y$ cannot be negative, and $f$ should be capped above at e.g. 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up bounds on the K (intrinsics parameters)\n",
    "bounds = (np.full((params_size,),-np.inf),np.full((params_size,),np.inf))\n",
    "bounds[0][K_idx:K_idx+3] = [100,0,0]\n",
    "bounds[1][K_idx:K_idx+3] = [800,mm.images[0].shape[1],mm.images[0].shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the NLLSQ optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = scipy.optimize.least_squares(calcResidualsBA, x0, \n",
    "                                   jac_sparsity=A, \n",
    "                                   verbose=2, \n",
    "                                   x_scale='jac', \n",
    "                                   ftol=1e-5, \n",
    "                                   jac='3-point',\n",
    "                                   bounds=bounds,\n",
    "                                   args=(n_cams, n_pts, [3, 5, 7], mpts2DForViews, visibility, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look on the effect the optimization had on the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(calcResidualsBA(x0.astype(np.float64), n_cams, n_pts, [3, 5, 7], mpts2DForViews, visibility, False),label='original')\n",
    "plt.plot(calcResidualsBA(res.x, n_cams, n_pts, [3, 5, 7], mpts2DForViews, visibility, False),label='optimized')\n",
    "plt.title('Residuals')\n",
    "plt.legend(fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems the residuals have mostly all improved.\n",
    "\n",
    "Now visually on the images with the points projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "calcResidualsBA(res.x, n_cams, n_pts, [3, 5, 7], mpts2DForViews, visibility, True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better than before, right?\n",
    "\n",
    "Collect the optimized measurements from the parameters vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack the results vector.\n",
    "# save in the variables `pts3d_hat`, `ts_hat`, `Rs_hat`, `K_hat`\n",
    "# the location of these parameters in the results vector is the same as in the sparsity matrix and as before.\n",
    "\n",
    "Rs_hat    = res.x[ 0        : 3*n_cams     ].reshape((n_cams,3))\n",
    "ts_hat    = res.x[ 3*n_cams : 6*n_cams     ].reshape((n_cams,3))\n",
    "K_hat     = res.x[ 6*n_cams : 6*n_cams + 3 ]\n",
    "pts3d_hat = res.x[ 6*n_cams + 3 : ].reshape((n_pts,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the 3D points (top-view, XZ plane):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results to see if they make sense\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(pts3d_hat[:,0],pts3d_hat[:,2], label='3D points')\n",
    "for cam_i in range(n_cams):\n",
    "    Ri = cv2.Rodrigues(Rs_hat[cam_i])[0]\n",
    "    ti = ts_hat[cam_i]\n",
    "    ti = -Ri.T @ ti\n",
    "    Ri = Ri.T\n",
    "    plt.scatter(ti[0],ti[2],s=100, label='Cameras')\n",
    "    plt.quiver(ti[0],ti[2],Ri[0,2],Ri[0,0],label='Direction')\n",
    "\n",
    "plt.xlabel('X', fontsize='xx-large'),plt.ylabel('Z', fontsize='xx-large')\n",
    "plt.xlim(-2,2),plt.ylim(-1,2.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a wrap!\n",
    "\n",
    "You have built a 3D reconstrution pipeline from scratch. This is a big deal!\n",
    "\n",
    "You can use this technique and extend it to work with images of your own, and make dense reconstructions with stereo matching (functions for which exist in OpenCV).\n",
    "\n",
    "We will see later the evolution of these methods in the world of deep learning and big datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
